import snkmk
import yaml
configfile: "config.yml"

shell.prefix = "set -xeuo pipefail; "

RUN2FILE = yaml.load(open("rawdata/samples.yml"))
SAMPLE2RUN = snkmk.make_sample2run()
VARCALL_REGIONS = snkmk.make_regions(config["refs"], window=config["varcall"]["chunksize"])
CHROMOSOMES = snkmk.make_chroms(config["refs"])
SAMPLESETS = snkmk.make_samplesets()

localrules: qc, map, varcall, angsd, denovodist, all
rule qc:
    input:
        expand("data/reads/samples/{sample}.fastq.gz", sample=SAMPLE2RUN),
        "data/readstats/readnum.tsv",
        "data/readstats/unique-kmers.tsv",

rule map:
    input:
        expand("data/alignments/{aligner}/{ref}/{sample}.bam",
               ref=config["mapping"]["ref"],
               aligner=config["mapping"]["aligners"],
               sample=SAMPLE2RUN),
        expand("data/alignments/{aligner}/{ref}_merged.bam",
               aligner=config["mapping"]["aligners"],
               ref=config["mapping"]["ref"]),

rule varcall:
    input:
        expand("data/variants/{caller}/{aligner}/{ref}.bcf",
               caller=config["varcall"]["callers"],
               aligner=config["varcall"]["aligners"],
               ref=config["varcall"]["genomes"]),

rule angsd:
    input:
        stat=expand("data/angsd/step1/{aligner}/{ref}/{set}/split/{region}.snpStat.gz",
                    aligner=config["angsd"]["aligners"], ref=config["angsd"]["genome"],
                    set=SAMPLESETS,  region=CHROMOSOMES[config["angsd"]["genome"]])

rule denovodist:
    input:
        expand("data/mash/k{ksize}-s{sketchsize}/{set}.dist",
                ksize=config["denovodist"]["ksize"],
                sketchsize=config["denovodist"]["mash_sketchsize"],
                set=[s for s, v in SAMPLESETS.items() if len(v) >=3]),
        expand("data/kwip/k{ksize}-s{sketchsize}/{set}.dist",
                ksize=config["denovodist"]["ksize"],
                sketchsize=config["denovodist"]["kwip_sketchsize"],
                set=[s for s, v in SAMPLESETS.items() if len(v) >=3]),


rule all:
    input:
        rules.qc.input,
        rules.map.input,
        rules.varcall.input,
        rules.angsd.input,
        rules.denovodist.input,


rule qcreads:
    input:
        r1=lambda wc: RUN2FILE[wc.run]["R1"],
        r2=lambda wc: RUN2FILE[wc.run]["R2"],
    output:
        reads="data/reads/runs/{run}.fastq.gz",
    log:
        log="data/log/adapterremoval/{run}.log",
        settings="data/stats/adapterremoval/{run}.txt",
    threads:
        8
    params:
        adp1=config["qc"]["adapter1"],
        adp2=config["qc"]["adapter2"],
        minqual=config["qc"]["minqual"],
    shell:
        "AdapterRemoval"
        "   --file1 <(cat {input.r1})"
        "   --file2 <(cat {input.r2})"
        "   --adapter1 {params.adp1}"
        "   --adapter2 {params.adp2}"
        "   --combined-output"
        "   --interleaved-output"
        "   --gzip"
        "   --collapse"
        "   --trimns"
        "   --trimqualities"
        "   --minquality {params.minqual}"
        "   --threads {threads}"
        "   --settings {log.settings}"
        "   --output1 {output.reads}"
        " >{log.log} 2>&1"

rule poolreads:
    input:
        reads=lambda wc: expand("data/reads/runs/{run}.fastq.gz", run=SAMPLE2RUN[wc.sample]),
    output:
        reads="data/reads/samples/{sample}.fastq.gz",
    threads: 16
    shell:
        "zcat {input} | pigz -p {threads} -7 >{output}"

rule read_count:
    input:
        expand("data/reads/runs/{run}.fastq.gz", run=RUN2FILE),
    output:
        "data/readstats/readnum.tsv",
    threads:
        16
    log:
        "data/log/readstats/seqhax-stats.log",
    shell:
        "( seqhax stats"
        "    -t {threads}"
        "    {input}"
        "    >{output}"
        " ) 2>{log}"

rule unique_kmers:
    input:
        expand("data/reads/runs/{run}.fastq.gz", run=RUN2FILE),
    output:
        "data/readstats/unique-kmers.tsv",
    threads:
        16
    params:
        kmersize=31,
    log:
        "data/log/readstats/unique-kmers.log",
    shell:
        "( kdm-unique-kmers.py"
        "    -t {threads}"
        "    -k {params.kmersize}"
        "    {input}"
        "    >{output}"
        " ) 2>{log}"



rule ngmap:
    input:
        reads="data/reads/samples/{sample}.fastq.gz",
        ref=lambda wc: config['refs'][wc.ref]
    output:
        bam="data/alignments/ngm/{ref}/{sample}.bam",
        bai="data/alignments/ngm/{ref}/{sample}.bam.bai",
    log:
        "data/log/ngm/{ref}/{sample}.log"
    threads:
        8
    shell:
        "( ngm"
        "   -q {input.reads}"
        "   -p" # paired input
        "   -r {input.ref}"
        "   -t {threads}"
        "   --rg-id {wildcards.sample}"
        "   --rg-sm {wildcards.sample}"
        "   --very-sensitive"
        " | samtools view -Suh -"
        " | samtools sort"
        "   -T ${{TMPDIR:-/tmp}}/{wildcards.sample}"
        "   -@ {threads}"
        "   -m 1G"
        "   -o {output.bam}"
        "   -" # stdin
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"

rule bwamem:
    input:
        reads="data/reads/samples/{sample}.fastq.gz",
        ref=lambda wc: config['refs'][wc.ref]
    output:
        bam="data/alignments/bwa/{ref}/{sample}.bam",
        bai="data/alignments/bwa/{ref}/{sample}.bam.bai",
    log:
        "data/log/bwa/{ref}/{sample}.log"
    threads:
        8
    shell:
        "( bwa mem"
        "   -p" # paired input
        "   -t {threads}"
        "   -R '@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}'"
        "   {input.ref}"
        "   {input.reads}"
        " | samtools view -Suh -"
        " | samtools sort"
        "   -T ${{TMPDIR:-/tmp}}/{wildcards.sample}"
        "   -@ {threads}"
        "   -m 1G"
        "   -o {output.bam}"
        "   -" # stdin
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"

rule mergebam:
    input:
        expand("data/alignments/{{aligner}}/{{ref}}/{sample}.bam", sample=SAMPLE2RUN),
    output:
        bam="data/alignments/{aligner}/{ref}_merged.bam",
        bai="data/alignments/{aligner}/{ref}_merged.bam.bai",
    log:
        "data/log/mergebam/{ref}.log"
    threads: 8
    shell:
        "( samtools merge"
        "   -@ {threads}"
        "   {output.bam}"
        "   {input}"
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"


rule freebayes:
    input:
        bam="data/alignments/{aligner}/{ref}_merged.bam",
        bai="data/alignments/{aligner}/{ref}_merged.bam.bai",
        ref=lambda wc: config['refs'][wc.ref],
    output:
        bcf="data/variants/freebayes/{aligner}/{ref}/split/{region}.bcf",
        idx="data/variants/freebayes/{aligner}/{ref}/split/{region}.bcf.csi",
    log:
        "data/log/freebayes/{aligner}/{ref}/{region}.log"
    threads: 1
    params:
        region=lambda wc: "' --region '".join(VARCALL_REGIONS[wc.ref][wc.region])
    shell:
        "( freebayes"
        "   --theta 0.02" # higher prior on mutation rate
        "   --use-reference-allele"
        "   --min-mapping-quality 10"
        "   --min-base-quality 10"
        "   --min-alternate-fraction 0.1"
        "   --min-alternate-count 1"
        "   --min-alternate-total 4"
        "   --use-mapping-quality"
        "   --genotype-qualities"
        "   --region '{params.region}'"
        "   -f {input.ref}"
        "   {input.bam}"
        " | bcftools view"
        "   -O b"
        "   -o {output.bcf}"
        " && bcftools index -f {output.bcf}"
        " ) >{log} 2>&1"

rule mpileup:
    input:
        bam="data/alignments/{aligner}/{ref}_merged.bam",
        bai="data/alignments/{aligner}/{ref}_merged.bam.bai",
        ref=lambda wc: config['refs'][wc.ref],
    output:
        bcf="data/variants/mpileup/{aligner}/{ref}/split/{region}.bcf",
        idx="data/variants/mpileup/{aligner}/{ref}/split/{region}.bcf.csi",
    log:
        "data/log/mpileup/{aligner}/{ref}/{region}.log"
    threads: 1
    params:
        region=lambda wc: "' --region '".join(VARCALL_REGIONS[wc.ref][wc.region]),
        targets=lambda wc: "' --targets '".join(VARCALL_REGIONS[wc.ref][wc.region]) # for bcftools
    shell:
        "( samtools mpileup"
        "   --output-tags DP,AD,ADF,ADR,SP,INFO/AD,INFO/ADF,INFO/ADR" #output everything
        "   --region '{params.region}'"
        "   --fasta-ref {input.ref}"
        "   --redo-BAQ"
        "   --BCF --uncompressed"
        "   {input.bam}"
        " | bcftools call"
        "   --targets '{params.targets}'" # might not be needed
        "   --multiallelic-caller"
        "   --prior 0.01" # increase mutation rate prior
        "   -O b"
        "   -o {output.bcf}"
        " && bcftools index -f {output.bcf}"
        " ) >{log} 2>&1"

rule bcfmerge:
    input:
        bcf=lambda wc: expand("data/variants/{caller}/{aligner}/{ref}/split/{region}.bcf",
                              caller=wc.caller, aligner=wc.aligner, ref=wc.ref,
                              region=sorted(VARCALL_REGIONS[wc.ref])),
        idx=lambda wc: expand("data/variants/{caller}/{aligner}/{ref}/split/{region}.bcf.csi",
                              caller=wc.caller, aligner=wc.aligner, ref=wc.ref,
                              region=sorted(VARCALL_REGIONS[wc.ref])),
    output:
        bcf="data/variants/{caller}/{aligner}/{ref}.bcf",
    log:
        "data/log/mergebcf/{caller}/{aligner}/{ref}.log"
    threads: 2
    shell:
        "( bcftools concat"
        "   --allow-overlaps"
        "   --remove-duplicates"
        "   --threads {threads}"
        "   -O b"
        "   -o {output.bcf}"
        "   {input.bcf}"
        " ) >{log} 2>&1"


#--------------------------------------------------------------------------------
#-                                    ANGSD                                     -
#--------------------------------------------------------------------------------


rule angsd_step1_split:
    input:
        bams=lambda wc: ["data/alignments/{aligner}/{ref}/{sample}.bam".format(
                             aligner=wc.aligner, ref=wc.ref, sample=s)
                         for s in sorted(SAMPLESETS[wc.set])],
        ref=lambda wc: config['refs'][wc.ref],
    output:
        arg="data/angsd/step1/{aligner}/{ref}/{set}/split/{region}.arg",
        hwe="data/angsd/step1/{aligner}/{ref}/{set}/split/{region}.hwe.gz",
        stat="data/angsd/step1/{aligner}/{ref}/{set}/split/{region}.snpStat.gz",
    log:
        "data/log/angsd/step1/{aligner}/{ref}/{set}/{region}.log"
    params:
        regionarg=lambda wc: "" if wc.region == "genomewide" else "-rf $T/regions",
        regions=lambda wc: " ".join(CHROMOSOMES[wc.ref][wc.region]),
        gl=config["angsd"].get("glmethod", 2),
        minind=config["angsd"].get("minind", 1),
        mindp=config["angsd"].get("mindepth", 1),
        maxdp=config["angsd"].get("maxdepth", 1000),
        snppval=config["angsd"].get("snppval", 1/1000),
        minq=config["angsd"].get("minq", 1),
        minmapq=config["angsd"].get("minmapq", 1),
    threads: 2
    shell:
        "T=$(mktemp -d); trap \"rm -rf $T\" EXIT &&"
        "echo {input.bams} | tr ' ' '\\n' > $T/bamfile && "
        "echo {params.regions} | tr ' ' '\\n' > $T/regions && "
        "( angsd"
        "   -bam $T/bamfile"
        "   {params.regionarg}"
        "   -P {threads}"
        "   -doCounts 1"
        "   -doMaf 1"
        "   -doMajorMinor 1"
        "   -doSNPStat 1"
        "   -baq 1"
        "   -anc {input.ref}"
        "   -ref {input.ref}"
        "   -out $(dirname {output.arg})/$(basename {output.arg} .arg)"
        "   -GL {params.gl}"
        "   -snp_pval {params.snppval}"
        "   -minMapQ {params.minmapq}"
        "   -minQ  {params.minq}"
        "   -skipTriallelic 1"
        " ) >{log} 2>&1"


#--------------------------------------------------------------------------------
#-                               de novo distance                               -
#--------------------------------------------------------------------------------

rule mashsketch:
    input:
        lambda wc: expand("data/reads/samples/{sample}.fastq.gz",
                          sample=SAMPLESETS[wc.set]),
    output:
        "data/mash/k{ksize}-s{sketchsize}/{set}.msh"
    log:
        "data/log/mash/sketch/k{ksize}-s{sketchsize}-{set}.log"
    threads: 16
    shell:
        " mash sketch"
        "   -k {wildcards.ksize}"
        "   -s {wildcards.sketchsize}"
        "   -p {threads}"
        "   -o data/mash/k{wildcards.ksize}-s{wildcards.sketchsize}/{wildcards.set}"
        "   {input}"
        " >{log} 2>&1"


rule mash:
    input:
        "data/mash/k{ksize}-s{sketchsize}/{set}.msh"
    output:
        dist="data/mash/k{ksize}-s{sketchsize}/{set}.mashdist",
    log:
        "data/log/mash/dist/k{ksize}-s{sketchsize}-{set}.log"
    threads: 16
    shell:
        "mash dist"
        "   -p {threads}"
        "   {input} {input}" # needs it twice
        " >{output}"
        " 2>{log}"

localrules: mashdist
rule mashdist:
    input:
        "data/mash/k{ksize}-s{sketchsize}/{set}.mashdist"
    output:
        "data/mash/k{ksize}-s{sketchsize}/{set}.dist"
    run:
        from collections import defaultdict
        from os.path import basename
        def fname2id(fname):
            fname = basename(fname)
            exts = [".gz", ".fastq", ".fq"]
            for ext in exts:
                if fname.endswith(ext):
                    fname = fname[:-len(ext)]
            return fname

        dists = defaultdict(dict)
        with open(input[0]) as fh:
            for line in fh:
                dist = line.strip().split('\t')
                id1 = fname2id(dist[0])
                id2 = fname2id(dist[1])
                dist = float(dist[2])
                dists[id1][id2] = dist

        with open(output[0], 'w') as ofile:
            ids = [''] + list(sorted(dists.keys()))
            print(*ids, sep='\t', file=ofile)
            for id1, row in sorted(dists.items()):
                rowdists = [it[1] for it in sorted(row.items())]
                print(id1, *rowdists, sep='\t', file=ofile)

rule countsketch:
    input:
        "data/reads/samples/{sample}.fastq.gz",
    output:
        ct="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz",
        info="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz.info",
        tsv="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz.info.tsv",
    log:
        "data/log/kwip/sketch/k{ksize}-s{sketchsize}-{sample}.log"
    threads:
        8
    shell:
        "load-into-counting.py"
        "   -N 1"
        "   -x {wildcards.sketchsize}"
        "   -k {wildcards.ksize}"
        "   -b"
        "   -f"
        "   -s tsv"
        "   -T {threads}"
        "   {output.ct}"
        "   {input}"
        " >{log} 2>&1"

rule kwip:
    input:
        lambda wc: expand("data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz",
                            ksize=wc.ksize, sketchsize=wc.sketchsize,
                            sample=SAMPLESETS[wc.set]),
    output:
        d="data/kwip/k{ksize}-s{sketchsize}/{set}.dist",
        k="data/kwip/k{ksize}-s{sketchsize}/{set}.kern",
    log:
        "data/log/kwip/dist/k{ksize}-s{sketchsize}-{set}.log"
    threads:
        8
    shell:
        "kwip"
        " -d {output.d}"
        " -k {output.k}"
        " -t {threads}"
        " {input}"
        " >{log} 3>&1"
