import snkmk
import yaml
configfile: "config.yml"

shell.prefix = "set -xeuo pipefail; "

LIB2SAMP, SAMP2LIB = snkmk.make_lib2sample2lib()
VARCALL_REGIONS = snkmk.make_regions(config["refs"], window=config["varcall"]["chunksize"])
CHROMOSOMES = snkmk.make_chroms(config["refs"])
SAMPLESETS = {"all_samples": list(SAMP2LIB.keys())}

localrules: qc, map, varcall, denovodist, all

rule qc:
    input:
        expand("data/reads/samples/{sample}.fastq.gz", sample=SAMP2LIB),
        "data/readstats/readnum.tsv",
        "data/readstats/unique-kmers.tsv",

rule map:
    input:
        expand("data/alignments/{aligner}/{ref}/{sample}.bam",
               ref=config["mapping"]["ref"],
               aligner=config["mapping"]["aligners"],
               sample=SAMP2LIB)

rule varcall:
    input:
        expand("data/variants/{caller}/{aligner}/{ref}/{sampleset}.bcf",
               caller=config["varcall"]["callers"],
               aligner=config["varcall"]["aligners"],
               ref=config["varcall"]["genomes"],
               sampleset="all_samples"),


rule denovodist:
    input:
        expand("data/mash/k{ksize}-s{sketchsize}/{set}.dist",
                ksize=config["denovodist"]["ksize"],
                sketchsize=config["denovodist"]["mash_sketchsize"],
                set=[s for s, v in SAMPLESETS.items() if len(v) >=3]),
        expand("data/kwip/k{ksize}-s{sketchsize}/{set}.dist",
                ksize=config["denovodist"]["ksize"],
                sketchsize=config["denovodist"]["kwip_sketchsize"],
                set=[s for s, v in SAMPLESETS.items() if len(v) >=3]),


rule all:
    input:
        rules.qc.input,
        rules.map.input,
        rules.varcall.input,
        rules.denovodist.input,


localrules: qcreads
rule qcreads:
    input:
        r1="rawdata/libraries/{lib}_R1.fastq.gz",
        r2="rawdata/libraries/{lib}_R2.fastq.gz",
    output:
        reads="data/reads/libraries/{lib}.fastq.gz",
    log:
        log="data/log/adapterremoval/{lib}.log",
        settings="data/stats/adapterremoval/{lib}.txt",
    threads:
        2
    params:
        adp1=config["qc"]["adapter1"],
        adp2=config["qc"]["adapter2"],
        minqual=config["qc"]["minqual"],
    shell:
        "( AdapterRemoval"
        "   --file1 {input.r1}"
        "   --file2 {input.r2}"
        "   --adapter1 {params.adp1}"
        "   --adapter2 {params.adp2}"
        "   --combined-output"
        "   --interleaved-output"
        "   --trimns"
        "   --trimqualities"
        "   --trimwindows 10"
        "   --minquality {params.minqual}"
        "   --threads {threads}"
        "   --settings {log.settings}"
        "   --output1 /dev/stdout"
        " | seqhax pairs"
        "   -l 20"
        "   -b >(gzip >{output.reads})"
        "   /dev/stdin"
        ") >{log.log} 2>&1"

localrules: poolreads
rule poolreads:
    input:
        reads=lambda wc: expand("data/reads/libraries/{lib}.fastq.gz", lib=SAMP2LIB[wc.sample]),
    output:
        reads="data/reads/samples/{sample}.fastq.gz",
    threads: 1
    shell:
        "cat {input} >{output}"

rule read_count:
    input:
        expand("data/reads/libraries/{run}.fastq.gz", run=LIB2SAMP),
    output:
        "data/readstats/readnum.tsv",
    threads:
        16
    log:
        "data/log/readstats/seqhax-stats.log",
    shell:
        "( seqhax stats"
        "    -t {threads}"
        "    {input}"
        "    >{output}"
        " ) 2>{log}"

rule unique_kmers:
    input:
        expand("data/reads/libraries/{run}.fastq.gz", run=LIB2SAMP),
    output:
        "data/readstats/unique-kmers.tsv",
    threads:
        16
    params:
        kmersize=31,
    log:
        "data/log/readstats/unique-kmers.log",
    shell:
        "( kdm-unique-kmers.py"
        "    -t {threads}"
        "    -k {params.kmersize}"
        "    {input}"
        "    >{output}"
        " ) 2>{log}"

rule ngmap:
    input:
        reads="data/reads/samples/{sample}.fastq.gz",
        ref=lambda wc: config['refs'][wc.ref]
    output:
        bam="data/alignments/ngm/{ref}/{sample}.bam",
        bai="data/alignments/ngm/{ref}/{sample}.bam.bai",
    log:
        "data/log/ngm/{ref}/{sample}.log"
    threads:
        8
    shell:
        "( ngm"
        "   -q {input.reads}"
        "   -p" # paired input
        "   -r {input.ref}"
        "   -t {threads}"
        "   --rg-id {wildcards.sample}"
        "   --rg-sm {wildcards.sample}"
        "   --very-sensitive"
        " | samtools view -Suh -"
        " | samtools sort"
        "   -T ${{TMPDIR:-/tmp}}/{wildcards.sample}"
        "   -@ {threads}"
        "   -m 1G"
        "   -o {output.bam}"
        "   -" # stdin
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"

rule bwamem:
    input:
        reads="data/reads/samples/{sample}.fastq.gz",
        ref=lambda wc: config['refs'][wc.ref]
    output:
        bam="data/alignments/bwa/{ref}/{sample}.bam",
        bai="data/alignments/bwa/{ref}/{sample}.bam.bai",
    log:
        "data/log/bwa/{ref}/{sample}.log"
    threads:
        8
    shell:
        "( bwa mem"
        "   -p" # paired input
        "   -t {threads}"
        "   -R '@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}'"
        "   {input.ref}"
        "   {input.reads}"
        " | samtools view -Suh -"
        " | samtools sort"
        "   -T ${{TMPDIR:-/tmp}}/{wildcards.sample}"
        "   -@ {threads}"
        "   -m 1G"
        "   -o {output.bam}"
        "   -" # stdin
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"

rule mergebam:
    input:
        expand("data/alignments/{{aligner}}/{{ref}}/{sample}.bam", sample=SAMP2LIB),
    output:
        bam="data/alignments/{aligner}/{ref}_merged.bam",
        bai="data/alignments/{aligner}/{ref}_merged.bam.bai",
    log:
        "data/log/mergebam/{ref}.log"
    threads: 2
    shell:
        "( samtools merge"
        "   -@ {threads}"
        "   {output.bam}"
        "   {input}"
        " && samtools index {output.bam}"
        " ) >{log} 2>&1"


localrules: bamlist
rule bamlist:
    input:
        lambda wc: expand("data/alignments/{aligner}/{ref}/{sample}.bam",
                          aligner=wc.aligner, ref=wc.ref, sample=SAMPLESETS[wc.sampleset]),

    output:
        "data/bamlists/{aligner}/{ref}/{sampleset}.bamlist",
    run:
        with open(output[0], "w") as fh:
            for s in input:
                print(s, file=fh)


localrules: freebayes
rule freebayes:
    input:
        bamlist="data/bamlists/{aligner}/{ref}/{sampleset}.bamlist",
        ref=lambda wc: config['refs'][wc.ref],
    output:
        bcf=temp("data/variants/freebayes/{aligner}/{ref}/{sampleset}/split/{region}.bcf"),
        idx=temp("data/variants/freebayes/{aligner}/{ref}/{sampleset}/split/{region}.bcf.csi"),
    log:
        "data/log/freebayes/{aligner}/{ref}/{sampleset}/{region}.log"
    threads: 1
    params:
        region=lambda wc: "' --region '".join(VARCALL_REGIONS[wc.ref][wc.region]),
        theta=config["varcall"].get("theta_prior", 0.01),
    shell:
        "( freebayes"
        "   --theta {params.theta}"
        "   --use-reference-allele"
        "   --min-mapping-quality 10"
        "   --min-base-quality 10"
        "   --min-alternate-fraction 0.1"
        "   --min-alternate-count 1"
        "   --min-alternate-total 4"
        "   --use-mapping-quality"
        "   --genotype-qualities"
        "   --region '{params.region}'"
        "   --fasta-reference {input.ref}"
        "   --bam-list {input.bamlist}"
        " | bcftools view"
        "   -O b"
        "   -o {output.bcf}"
        " && bcftools index -f {output.bcf}"
        " ) >{log} 2>&1"

rule mpileup:
    input:
        bamlist="data/bamlists/{aligner}/{ref}/{sampleset}.bamlist",
        ref=lambda wc: config['refs'][wc.ref],
    output:
        bcf=temp("data/variants/mpileup/{aligner}/{ref}/{sampleset}/split/{region}.bcf"),
        idx=temp("data/variants/mpileup/{aligner}/{ref}/{sampleset}/split/{region}.bcf.csi"),
    log:
        "data/log/mpileup/{aligner}/{ref}/{sampleset}/{region}.log"
    threads: 1
    params:
        region=lambda wc: "' --region '".join(VARCALL_REGIONS[wc.ref][wc.region]),
        targets=lambda wc: "' --targets '".join(VARCALL_REGIONS[wc.ref][wc.region]), # for bcftools
        theta=config["varcall"].get("theta_prior", 0.01),
    shell:
        "( bcftools mpileup"
        "   --output-tags DP,AD,ADF,ADR,SP,INFO/AD,INFO/ADF,INFO/ADR" #output everything
        "   --region '{params.region}'"
        "   --fasta-ref {input.ref}"
        "   --bam-list {input.bamlist}"
        "   --redo-BAQ"
        "   --BCF --uncompressed"
        " | bcftools call"
        "   --targets '{params.targets}'" # might not be needed
        "   --multiallelic-caller"
        "   --prior {params.theta}"
        "   -O b"
        "   -o {output.bcf}"
        " && bcftools index -f {output.bcf}"
        " ) >{log} 2>&1"

rule bcfmerge:
    input:
        bcf=lambda wc: expand("data/variants/{caller}/{aligner}/{ref}/{sampleset}/split/{region}.bcf",
                              caller=wc.caller, aligner=wc.aligner, ref=wc.ref, sampleset=wc.sampleset,
                              region=sorted(VARCALL_REGIONS[wc.ref])),
        idx=lambda wc: expand("data/variants/{caller}/{aligner}/{ref}/{sampleset}/split/{region}.bcf.csi",
                              caller=wc.caller, aligner=wc.aligner, ref=wc.ref, sampleset=wc.sampleset,
                              region=sorted(VARCALL_REGIONS[wc.ref])),
    output:
        bcf="data/variants/{caller}/{aligner}/{ref}/{sampleset}.bcf",
        idx="data/variants/{caller}/{aligner}/{ref}/{sampleset}.bcf.csi",
    log:
        "data/log/mergebcf/{caller}/{aligner}/{ref}/{sampleset}.log"
    threads: 2
    shell:
        "( bcftools concat"
        "   --allow-overlaps"
        "   --remove-duplicates"
        "   --threads {threads}"
        "   -O b"
        "   -o {output.bcf}"
        "   {input.bcf}"
        " && bcftools index -f {output.bcf}"
        " ) >{log} 2>&1"

#--------------------------------------------------------------------------------
#-                               de novo distance                               -
#--------------------------------------------------------------------------------

rule mashsketch:
    input:
        lambda wc: expand("data/reads/samples/{sample}.fastq.gz",
                          sample=SAMPLESETS[wc.set]),
    output:
        "data/mash/k{ksize}-s{sketchsize}/{set}.msh"
    log:
        "data/log/mash/sketch/k{ksize}-s{sketchsize}-{set}.log"
    threads: 16
    shell:
        " mash sketch"
        "   -k {wildcards.ksize}"
        "   -s {wildcards.sketchsize}"
        "   -p {threads}"
        "   -o data/mash/k{wildcards.ksize}-s{wildcards.sketchsize}/{wildcards.set}"
        "   {input}"
        " >{log} 2>&1"


rule mash:
    input:
        "data/mash/k{ksize}-s{sketchsize}/{set}.msh"
    output:
        dist="data/mash/k{ksize}-s{sketchsize}/{set}.mashdist",
    log:
        "data/log/mash/dist/k{ksize}-s{sketchsize}-{set}.log"
    threads: 16
    shell:
        "mash dist"
        "   -p {threads}"
        "   {input} {input}" # needs it twice
        " >{output}"
        " 2>{log}"

localrules: mashdist
rule mashdist:
    input:
        "data/mash/k{ksize}-s{sketchsize}/{set}.mashdist"
    output:
        "data/mash/k{ksize}-s{sketchsize}/{set}.dist"
    run:
        from collections import defaultdict
        from os.path import basename
        def fname2id(fname):
            fname = basename(fname)
            exts = [".gz", ".fastq", ".fq"]
            for ext in exts:
                if fname.endswith(ext):
                    fname = fname[:-len(ext)]
            return fname

        dists = defaultdict(dict)
        with open(input[0]) as fh:
            for line in fh:
                dist = line.strip().split('\t')
                id1 = fname2id(dist[0])
                id2 = fname2id(dist[1])
                dist = float(dist[2])
                dists[id1][id2] = dist

        with open(output[0], 'w') as ofile:
            ids = [''] + list(sorted(dists.keys()))
            print(*ids, sep='\t', file=ofile)
            for id1, row in sorted(dists.items()):
                rowdists = [it[1] for it in sorted(row.items())]
                print(id1, *rowdists, sep='\t', file=ofile)

rule countsketch:
    input:
        "data/reads/samples/{sample}.fastq.gz",
    output:
        ct="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz",
        info="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz.info",
        tsv="data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz.info.tsv",
    log:
        "data/log/kwip/sketch/k{ksize}-s{sketchsize}-{sample}.log"
    threads:
        4
    shell:
        "load-into-counting.py"
        "   -N 1"
        "   -x {wildcards.sketchsize}"
        "   -k {wildcards.ksize}"
        "   -b"
        "   -f"
        "   -s tsv"
        "   -T {threads}"
        "   {output.ct}"
        "   {input}"
        " >{log} 2>&1"

rule kwip:
    input:
        lambda wc: expand("data/kwip/sketch/k{ksize}-s{sketchsize}/{sample}.ct.gz",
                            ksize=wc.ksize, sketchsize=wc.sketchsize,
                            sample=SAMPLESETS[wc.set]),
    output:
        d="data/kwip/k{ksize}-s{sketchsize}/{set}.dist",
        k="data/kwip/k{ksize}-s{sketchsize}/{set}.kern",
    log:
        "data/log/kwip/dist/k{ksize}-s{sketchsize}-{set}.log"
    threads:
        8
    shell:
        "kwip"
        " -d {output.d}"
        " -k {output.k}"
        " -t {threads}"
        " {input}"
        " >{log} 3>&1"
